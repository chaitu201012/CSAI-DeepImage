% This is a borrowed LaTeX template file for lecture notes for CS267,
% Applications of Parallel Computing, UCBerkeley EECS Department.

% To familiarize yourself with this template, the body contains
% some examples of its use.  Look them over.

\documentclass[a4paper]{article}

%
% ADD PACKAGES here:
%
\usepackage{amsmath,amsfonts,graphicx,multicol}
\usepackage{natbib}
\usepackage{blindtext}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage[margin=1in]{geometry}
\usepackage{amsthm}

\setlength{\parskip}{\baselineskip} % Set space between paras
\setlength{\parindent}{0pt} % No para indentation

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}

\begin{document}

%% Header

\pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \noindent
   \begin{center}
   \framebox
   {
      \vbox{\vspace{2mm}
        \hbox to 6.28in { {\bf CS9.432: Cognitive Science And AI
	    \hfill Spring 2022} }
      \vspace{4mm}
        \hbox to 6.28in { {\Large \hfill Deep Image Reconstruction from Human Brain Activity \hfill} } %% Fill here
        \vspace{2mm}
        \hbox to 6.28in { {\it Team Helper Func: Chaitanya Kumar P(2020201012), Mounika Kanakanti (2019900003) \hfill} } %% Fill here
        \vspace{2mm}
        \hbox {{\it \hspace{20mm} Ankitha Reddy P ( 2020900007) \hfill }}
         \vspace{2mm}}
   }
   \end{center}
   \markboth{CNN}{CNN}
   %% Fill here


% \maketitle
\section{Introduction:}

There is some converging evidence that visual stimuli like imagery are encoded in hierarchical representations in the brain. Many attempts have been made to visualize the contents from the perception. But the problem is they failed to use the multiple levels of hierarchy. Recent advancements in the neuro imaging techniques has showed that visual cortical activity can be captured using fMRI techniques and perception of visual stimuli can be decoded into hierarchical features. 
\\
\\
Earlier studies failed to reconstruct the images from decoding the representation since the visual imagery is encoded in the visual cortex in hierarchical way. In this project we will try to replicate the paper by Yukiyasu Kamitani. In this they described a novel approach to capture the hierarchical features and to reconstruct the images as close to the visual perception they used vgg-19 pretrained neural network. Earlier studies conducted by kamitani labs proved that the patterns generated by the human brain activity and the vgg-19 are similar and are homological in multiple layers. So we use the vgg-19 to reconstruct the image.
\\
\section{ Approach:}
In this novel approach a decoder has been constructed for each unit i in the layer j to predict the output that unit. The features that obtained from decoders are then used to reconstruct the image which is as close to the image as possible.  The image was optimised in such a way the dnn features look similar to that of the values decoded from the brain activity. A Deep Generator Network(DGN) is also used to confine the image to fall in the latent space of the particular image. 
\\
\\
In order to reconstruct the image from human brain activity we have taken three pre-trained neural networks namely vgg19 as the base model and vgg16, ResNet50 as additional pre-trained models to compare the quality of the reconstructed image obtained from the vgg19. vgg19 dnn was trained on ImageNet database which consist of 1000 object categories. The max pool layers are replaced with the average pool layers. 


\section{Experimental setup:}

    \subsection{Participants:}
    There healthy participants among which two were male and one female whose ages are in the range of 23-33 were chosen for the study. The experiment consist of multiple runs where in each run a subject is supposed to view 50 - 60 images which are in turn chosen from ImageNet.
   \\
   \\
    \subsection{Data:}
     While subjects are viewing images their perception of the stimuli is captured through fMRI scans and stored. In total each subject was trained on 1200 images. Each set was again repeated 4 more times so totalling of 6000 images and scans. only 1200 images are unique and the same were repeated again. For test data 50 images were used and 24 sets are done, so a total of 1200 test images are used among. The same data can be requested \href{https://forms.gle/c6HGatLrt7JtTGQk7}{here} along with their sequential order in csv files.
     \\
     \\
     \subsection{Architecture:}
     
     Architecture consists of a vgg19 dnn and a set of regression models as decoders and a DGN. The whole architecture can be seen in the following figure.
     \begin{figure}[ht]
    \centering
    \includegraphics[scale=1.0]{image_architecture.png}
    \caption{Architecture}
    \end{figure}
    We can see that the input image is fed into the feature decoder and the decoded features thus obtained are sent to optimizer that takes in the latent space from the dgn also as one of the input and tries to reduce the error. After multiple iterations the reconstructed image resembles close to the stimuli. 
    
    



% \maketitle

\section{Decoder:}

    In this approach a decoder is constructed for each unit in each layer. Let us consider the 9th of the layer in the vgg19 pretrained model. It has the dimensions as 56x56x256, so a total of 8,02,816 units. So decoding features for each unit takes a lot of time. Preferred system requirements for the same is 60gb ram with gpu enabled and 8 cpus.  After the features were decoded they are saved for later reconstruction. 
    
    \\
    Similarly we used the vgg16 for decoding and the features obtained were saved. It was established that mid-level visual features are crucial in the perception of stimuli. DNN layers here act as feature generators using the fMRI data as the train data to train the decoder. Later these decoded features were used to reconstruct the image. These feature decoders were trained only on fMRI data obtained from natural images.
    \\
    \\
    \begin{figure}[ht]
    \centering
    \includegraphics[scale=1.0]{decoding.png}
    \caption{Decoder}
    \end{figure}
    In this, we used Sparse Linear Regression (SLR) which can automatically select the important voxels for decoding. Sparsity was introduced and decoder features are again normalized so that the mean norm should be equal to that of the feature vectors computed from training 10000 natural images 
\section {Reconstruction Algorithm:}

Once we map the fMRI signals to the decoders, we try to generate the image that would create those same features given a pre-trained model. Instead of optimizing the weights of  the pre-trained network it will try to optimize the image variable, initialized as all 0's or some random values. If we use DGN, then gradient descent is applied, otherwise L-BFGS is used as optimizer to get an image that will be produce features as close as possible to the features decoded from fMRI signals.




\section{Decoder Per Layer:}
Apart from the decoder per unit we tried to realize the quality of the reconstruction of the image by constructing one decoder per layer. 
We flattened out the feature maps and the decoder was constructed with the fMRI data as the feature vector and the flattened maps as the target data. So we thought of experimenting with layer wise decoders to observe the subjective quality of the reconstructed image.  One more problem with the existing code is it is completely built on caffe and they used their own libraries . So using tensorflow or pytorch would be a novel approach.We used vgg19 and ResNet50 as pretrained models. Further we compared the results . For decoding we used ridgecv regressor to train the layer wise decoders.fMRI data is used as the X data and the corresponding natural image is used as the Y_data. 
\\

Decoder outputs from prediction on the test data were reconstructed into the images to compare the quality. It has been observed that the mid layers are one of the crucial layers in the visual cortex hierarchy. So we tried to visualize some mid layers and last layer in the hierarchy of the dnn. since the feature maps when flattened out are of huge dimensions it is difficult to  train decoders for long dimensional target vectors. We used google cloud engine as the run time environment for the google colab.  Code can be obtained \href{https://github.com/chaitu201012/CSAI-DeepImage}{here}


\section {Results:}

\subsection {Reconstruction with and with out DGN:}
We first visualize a few sample outputs of our method for both without and with generator models for the same input. We can observe from the following figure that the one with out DGN does not produce images relevant to the actual stimuli, it only shows some silhouette and edges. But when it comes to images reconstructed using the DGN has some resemblance to the actual stimuli despite not exact looking subjectively a person can identify that it resembles some animal or object. 
\begin{figure}[ht]
  \centering
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{results1.png}
    \caption{Image001.}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{results2.png}
    \caption{Image002.}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{results3.png}
    \caption{Image009.}
  \end{minipage}
\end{figure}
\\
\subsection {Reconstruction and comparison with vgg16 and vgg19(base):}
We first visualize a few sample outputs of our method for both vgg16 and  vgg19 models for the same input. We can observe from the following figure that both the vgg16 and vgg19 has some good looking images but the vgg19 captures the shape and the similarity to the actual image  and similarity is higher where as vgg16 roughly produced some shapes of animals and birds. we can see the eyes, beak , fur etc, but the resemblance is not high compared to the vgg19's image. 


\begin{figure}[ht]
  \centering
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{results4.png}
    \caption{Image001.}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{results5.png}
    \caption{Image002.}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{results6.png}
    \caption{Image009.}
  \end{minipage}
\end{figure}
\\
\subsection {Reconstruction from layer wise decoding :}
We first visualize a few sample outputs of our method for some layers namely layer 12 and layer 19. We can observe that the images reconstructed are much similar indicating that the middle layers are also important in the perception of the stimuli. But the images no where looks similar to the stimuli. This also prove that layer wise decoding alone is not enough and we need to do decoding unit wise to entirely capture the layer wise features.  

\begin{figure}[ht]
  \centering
  \begin{minipage}[b]{0.5\textwidth}
    \includegraphics[width=\textwidth]{results7.png}
    \caption{Image002.}
  \end{minipage}
\end{figure}
\\
\subsection {Reconstruction from layer wise decoding vgg19 vs ResNet50 :}
We first visualize a few sample outputs of our method for last layer of the vgg19 and resnet50. We can clearly observe that the images reconstructed from the vgg19 atleast has some silhouettes and shapes even though they are not similar to the stimuli but when it comes to the resnet50 image, it does not even produce some shapes. This clearly shows that the resnet is not suitable for this task because of very deep architecture (~170 layers). 


\begin{figure}[ht]
  \centering
  \begin{minipage}[b]{0.5\textwidth}
    \includegraphics[width=\textwidth]{results8.png}
    \caption{Image002.}
  \end{minipage}
\end{figure}
\\
\subsection{Objective parameters:}

The following tables describe pixel wise spatial correlation between the original image and the reconstructed image and compared against several models.vgg19 has better correlation for several images and can be observed for the sampled images also. Similarly pixel wise correlation with DGN is better compared to the one with out DGN

 
\begin{figure}[ht]
  \centering
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{obj1.png}
    \caption{vgg16 vs vgg19}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{obj2.png}
    \caption{with and with out DGN.}
  \end{minipage}
\end{figure}

    


\section{Conclusion:}

Both the intermediate layer and final layer are detecting some shapes since we did not use any dgn to restrict the image, the shapes are not perfect.
When trained layer wise images from decoders are predicting some silhouettes and random edges which resembles some shape. Middle layers like layer 12 also produces some good shapes concluding that middle layers are important in visual representation of stimuli.
It is clear that the image predicted by the decoder trained for ResNet architecture is not good. It shows that some DNNs are not homological to the brain .
Only some of the pretrained models are similar to the brain like VGG16 and VGG19..
Both the VGG16 and VGG19 produced reasonable good images. 
VGG16 also gave good reconstructed images. They do capture shape information as well but not as well as VGG19. Spatial correlatin is higher for VGG19 

  
  \begin{thebibliography}{9}
  \bibitem{latex}  \href{https://doi.org/10.1371/journal.pcbi.1006633}{Deep Image Reconstruction From Human Brain Activity}
    \bibitem{latex} \href{https://www.nature.com/articles/ncomms15037}{Generic Object Decoding}
    \bibitem{latex} \href{https://github.com/KamitaniLab/DeepImageReconstruction}{Github code repo for reconstruction }
    \bibitem{latex} \href{https://github.com/KamitaniLab/dnn-feature-decoding}{Github code repo for feature decoding }
\end{thebibliography} 
    
\end{document}
